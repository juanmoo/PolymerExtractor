{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from bisect import bisect_left\n",
    "import random\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "pilot_docs = '/data/rsg/nlp/juanmoo1/projects/04_polymer/00_annotation/brat-v1.3_Crunchy_Frog/data/polymer/pilot_dylan/'\n",
    "bio_files_dir = '../data/all_reagent_grouping/bio_files/'\n",
    "kfold_split_dir = '../data/all_reagent_grouping/ner_evaluation/k_splits/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(bio_files_dir, exist_ok=True)\n",
    "# os.makedirs(split_dir, exist_ok=True)\n",
    "os.makedirs(kfold_split_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser BRAT Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotated_document_names(ann_path):\n",
    "    fnames = [e for e in os.listdir(ann_path) if e.lower().endswith(\".ann\") or e.lower().endswith(\".txt\")]\n",
    "    names = sorted(set([e.split('.')[0] for e in fnames]))\n",
    "    \n",
    "    names = [e for e in names if (f'{e}.txt' in fnames) and (f'{e}.ann' in fnames)]\n",
    "    names.sort()\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document_annotations(doc_name, ann_dir_path=pilot_docs):\n",
    "    \n",
    "    text_doc_path = os.path.join(ann_dir_path, f'{doc_name}.txt')\n",
    "    text = open(text_doc_path, 'r').read()\n",
    "    \n",
    "    ann_doc_path = os.path.join(ann_dir_path, f'{doc_name}.ann')\n",
    "    ann_lines = open(ann_doc_path, 'r').readlines()\n",
    "    \n",
    "    tokens = text.split()\n",
    "    tok_lens = [0] + list(map(len, tokens))\n",
    "    for j in range(1, len(tok_lens)):\n",
    "        tok_lens[j] += tok_lens[j - 1] + 1\n",
    "        \n",
    "    entities = []\n",
    "    links = []\n",
    "\n",
    "    \n",
    "    for line in ann_lines:\n",
    "        line = re.sub('\\s+', ' ', line).split()\n",
    "\n",
    "        ann_type = line[0]\n",
    "        ann_name = line[1]\n",
    "        \n",
    "        if ann_type.startswith('T'):\n",
    "            \n",
    "            chr_start, chr_stop = map(int, line[2:4])\n",
    "            ann_text = line[4]\n",
    "\n",
    "            tok_start = bisect_left(tok_lens, chr_start)\n",
    "            tok_stop = bisect_left(tok_lens, chr_stop)\n",
    "            \n",
    "            entities.append((ann_type, ann_name, tok_start, tok_stop))\n",
    "\n",
    "        elif ann_type.startswith('R'):\n",
    "            arg_1, arg_2 = map(lambda s: s.split(':')[1], line[2:4])\n",
    "            links.append((ann_type, ann_name, arg_1, arg_2))\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"doc_name\": doc_name,\n",
    "        \"text\": text,\n",
    "        \"spans\": entities,\n",
    "        \"links\": links\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_names = get_annotated_document_names(pilot_docs)\n",
    "parsed_annotations = [parse_document_annotations(dname) for dname in document_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BIO Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "all_entities = ['Monomer',\n",
    " 'Macromonomer',\n",
    " 'Initiator',\n",
    " 'Macroinitiator',\n",
    " 'Catalyst',\n",
    " 'Solvent',\n",
    " 'Reagent',\n",
    " 'Mixture',\n",
    " 'Product',\n",
    " 'Polymer',\n",
    " 'WorkupReagent',\n",
    " 'Lighting',\n",
    " 'RemovedChemical',\n",
    " 'ActionInput',\n",
    " 'Monomer_ref',\n",
    " 'Macromonomer_ref',\n",
    " 'Initiator_ref',\n",
    " 'Macroinitiator_ref',\n",
    " 'Catalyst_ref',\n",
    " 'Solvent_ref',\n",
    " 'Reagent_ref',\n",
    " 'Mixture_ref',\n",
    " 'Product_ref',\n",
    " 'Polymer_ref',\n",
    " 'WorkupReagent_ref',\n",
    " 'Lighting_ref',\n",
    " 'RemovedChemical_ref',\n",
    " 'ActionInput_ref',\n",
    " 'Vacuum_Condition',\n",
    " 'Atmosphere_Condition',\n",
    " 'Repetition_Condition',\n",
    " 'Action',\n",
    " 'Amount',\n",
    " 'Time',\n",
    " 'Temperature',\n",
    " 'Data',\n",
    " 'Yield']\n",
    "\n",
    "# identity_map = {e:e for e in all_entities}\n",
    "reagent_ents = ['Monomer', 'Macromonomer', 'Initiator', 'Macroinitiator', 'Catalyst', 'Solvent', 'Reagent', 'Polymer']\n",
    "reagent_map = [[(e, 'Reagent'), (e + '_ref', 'Reagent')] for e in reagent_ents]\n",
    "reagent_map.append([('ActionInput_ref', 'ActionInput'), ('WorkupReagent_ref', 'WorkupReagent')])\n",
    "reagent_map = dict(list(reduce(lambda a, b: a + b, reagent_map)))\n",
    "\n",
    "# all_reagents\n",
    "all_reagents = reagent_ents + ['WorkupReagent', 'RemovedChemical', 'ActionInput']\n",
    "all_reagents_map = [[(e, 'Reagent'), (e + '_ref', 'Reagent')] for e in all_reagents]\n",
    "all_reagents_map = dict(list(reduce(lambda a, b: a + b, all_reagents_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc_bio(par_dict, label_map=dict()):\n",
    "    tokens = par_dict['text'].split(' ')\n",
    "    labels = ['O'] * len(tokens)\n",
    "    \n",
    "    for _, key, s, t in par_dict['spans']:\n",
    "        ekey = label_map.get(key, key)\n",
    "        all_labels.add(ekey)\n",
    "        \n",
    "        if ekey == 'O':\n",
    "            # skip no-concept\n",
    "            break\n",
    "        \n",
    "        labels[s] = f'B-{ekey}'\n",
    "        for j in range(s + 1, t):\n",
    "            labels[j] = f'I-{ekey}'\n",
    "    \n",
    "    bio_par = []\n",
    "    for e in zip(tokens, labels):\n",
    "        bio_par.append('\\t'.join(e))\n",
    "    bio_par = '\\n'.join(bio_par)\n",
    "    \n",
    "    return (par_dict[\"doc_name\"], bio_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save files\n",
    "all_labels = set()\n",
    "bio_pars = [create_doc_bio(par_dict, all_reagents_map) for par_dict in parsed_annotations]\n",
    "\n",
    "for doc_name, bio_txt in bio_pars:\n",
    "    bio_file_path = os.path.join(bio_files_dir, f'{doc_name}.bio')\n",
    "    with open(bio_file_path, 'w') as bio_file:\n",
    "        bio_file.write(bio_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.8\n",
    "random.seed(0)\n",
    "paragraphs = [e[1] for e in bio_pars]\n",
    "random.shuffle(paragraphs)\n",
    "j = int(len(paragraphs) * train_percent)\n",
    "\n",
    "train_set = paragraphs[:j]\n",
    "test_set = paragraphs[j:]\n",
    "\n",
    "train_txt = '\\n\\n'.join(train_set)\n",
    "test_txt = '\\n\\n'.join(test_set)\n",
    "\n",
    "# with open(os.path.join(split_dir, 'train.txt'), 'w') as train_file:\n",
    "#     train_file.write(train_txt)\n",
    "\n",
    "# with open(os.path.join(split_dir, 'test.txt'), 'w') as test_file:\n",
    "#     test_file.write(test_txt)\n",
    "    \n",
    "# with open(os.path.join(split_dir, 'dev.txt'), 'w') as dev_file:\n",
    "#     dev_file.write(test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "all_labels = [e for e in all_labels if e != 'O']\n",
    "all_labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_labels = []\n",
    "for l in all_labels:\n",
    "    bio_labels.append(f'B-{l}')\n",
    "    bio_labels.append(f'I-{l}')\n",
    "bio_labels = ['O'] + bio_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-Action',\n",
       " 'I-Action',\n",
       " 'B-ActionInput',\n",
       " 'I-ActionInput',\n",
       " 'B-ActionInput_ref',\n",
       " 'I-ActionInput_ref',\n",
       " 'B-Amount',\n",
       " 'I-Amount',\n",
       " 'B-Atmosphere_Condition',\n",
       " 'I-Atmosphere_Condition',\n",
       " 'B-Lighting',\n",
       " 'I-Lighting',\n",
       " 'B-Macromonomer',\n",
       " 'I-Macromonomer',\n",
       " 'B-Product',\n",
       " 'I-Product',\n",
       " 'B-Reagent',\n",
       " 'I-Reagent',\n",
       " 'B-RemovedChemical',\n",
       " 'I-RemovedChemical',\n",
       " 'B-RemovedChemical_ref',\n",
       " 'I-RemovedChemical_ref',\n",
       " 'B-Repetition_Condition',\n",
       " 'I-Repetition_Condition',\n",
       " 'B-Temperature',\n",
       " 'I-Temperature',\n",
       " 'B-Time',\n",
       " 'I-Time',\n",
       " 'B-Vacuum_Condition',\n",
       " 'I-Vacuum_Condition',\n",
       " 'B-WorkupReagent',\n",
       " 'I-WorkupReagent',\n",
       " 'B-WorkupReagent_ref',\n",
       " 'I-WorkupReagent_ref',\n",
       " 'B-Yield',\n",
       " 'I-Yield']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bio_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "paragraphs = np.array(paragraphs)\n",
    "\n",
    "for j, (train_index, test_index) in enumerate(kf.split(paragraphs)):\n",
    "    split_path = os.path.join(kfold_split_dir, f'split_{j}')\n",
    "    os.makedirs(split_path, exist_ok=True)\n",
    "    \n",
    "    train_set = paragraphs[train_index]\n",
    "    test_set = paragraphs[test_index]\n",
    "    \n",
    "    train_txt = '\\n\\n'.join(train_set)\n",
    "    dev_txt = '\\n\\n'.join(test_set)\n",
    "    \n",
    "    with open(os.path.join(split_path, 'train.txt'), 'w') as train_file:\n",
    "        train_file.write(train_txt)\n",
    "        \n",
    "    with open(os.path.join(split_path, 'dev.txt'), 'w') as dev_file:\n",
    "        dev_file.write(dev_txt)\n",
    "    \n",
    "    with open(os.path.join(split_path, 'test.txt'), 'w') as test_file:\n",
    "        test_file.write(dev_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
