{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "pred_file_path = '/data/rsg/nlp/juanmoo1/projects/04_polymer/00_annotation/workdir/ner_evaluation/split/model_200/eval_predictions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paragraphs_from_file(path):\n",
    "    def clean_line(l):\n",
    "        l = re.sub('\\s+|\\t', ' ', l.replace('\\t', ' ')).strip()\n",
    "        return l\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    paragraphs = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        while i < len(lines) and len(clean_line(lines[i])) == 0:\n",
    "            i += 1\n",
    "\n",
    "        j = i + 1\n",
    "        while j < len(lines) and len(clean_line(lines[j])) > 0:\n",
    "            j += 1\n",
    "        paragraphs.append([clean_line(line) for line in lines[i:j]])\n",
    "        i = j\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Level Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, recall_score, precision_score, classification_report\n",
    "\n",
    "def get_report_from_pred_file(pred_file_path):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    paragraphs = load_paragraphs_from_file(pred_file_path)\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        if len(p) == 0:\n",
    "            continue\n",
    "        toks, labs, preds = zip(*[e.strip().split() for e in p if len(e.strip()) > 0])\n",
    "\n",
    "        toks = list(toks)\n",
    "        labs = list(labs)\n",
    "        preds = list(preds)\n",
    "\n",
    "        tokens.append(toks)\n",
    "        labels.append(labs)\n",
    "        predictions.append(preds)\n",
    "    \n",
    "    f1 = f1_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    report = classification_report(labels, predictions)\n",
    "    report = parse_report(report)\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_line(line):\n",
    "    line = re.sub('\\s+', ' ', line.strip())\n",
    "    return line.split()\n",
    "\n",
    "def parse_report(report):\n",
    "    lines = report.split('\\n')\n",
    "    \n",
    "    columns = parse_line(lines[0])\n",
    "    \n",
    "    \n",
    "    amounts = [parse_line(l) for l in lines[2:11]]\n",
    "    rows = []\n",
    "    for l in amounts:\n",
    "        label = l[0]\n",
    "        values = [float(e) for e in l[1:]]\n",
    "        \n",
    "        rows.append({\n",
    "            'label': label,\n",
    "            'precision': values[0],\n",
    "            'recall': values[1],\n",
    "            'f1-score': values[2],\n",
    "            'support': values[3]\n",
    "        })\n",
    "        \n",
    "    report = pd.DataFrame(rows)\n",
    "    return report.set_index('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_report_from_pred_file(pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "k_splits_dir = '/data/rsg/nlp/juanmoo1/projects/04_polymer/01_ner_bert/data/reagent_grouping/ner_evaluation/k_splits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_exp = os.path.join(k_splits_dir, '**', 'eval_predictions.txt')\n",
    "pred_files = glob(glob_exp, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rsg/nlp/juanmoo1/miniconda3/envs/jupyter/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data/rsg/nlp/juanmoo1/miniconda3/envs/jupyter/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for pred_file in pred_files:\n",
    "    results.append(get_report_from_pred_file(pred_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all labels\n",
    "label_set = set()\n",
    "for e in results:\n",
    "    report = e['report']\n",
    "    report_labels = set(report.index)\n",
    "    label_set |= report_labels\n",
    "label_set = sorted(list(label_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set non-existent values to zero\n",
    "for e in results:\n",
    "    report = e['report']\n",
    "    for l in label_set:\n",
    "        if (l not in report.index):\n",
    "            report = report.append(pd.Series(name=l, data={'precision': 0, 'recall':0, 'f1-score':0, 'support':0}))\n",
    "    report.sort_index(axis=0)\n",
    "    e['report'] = report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set non-existent values to zero\n",
    "for e in results:\n",
    "    report = e['report']\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average from reports\n",
    "reports = []\n",
    "for e in results:\n",
    "    report = e['report']\n",
    "    reports.append(report)\n",
    "report_sum = sum(reports)\n",
    "report_avg = report_sum/len(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Averages\n",
    "f1s = [r['f1'] for r in results]\n",
    "precisions = [r['recall'] for r in results]\n",
    "recalls = [r['precision'] for r in results]\n",
    "\n",
    "f1_avg = np.mean(f1s)\n",
    "f1_std = np.var(f1s)**0.5\n",
    "precision_avg = np.mean(precisions)\n",
    "precision_std = np.var(precisions)**0.5\n",
    "recall_avg = np.mean(recalls)\n",
    "recall_std = np.var(recalls)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold Cross Validation Results:\n",
      "\n",
      "Overall Statistics:\n",
      "\n",
      "F1-SCORE:\tMEAN: 0.8833767136052298\tSTD:0.025023695037861036\n",
      "PRECISION:\tMEAN: 0.9014315714871618\tSTD:0.01948830253633368\n",
      "RECALL:\t\tMEAN: 0.866261167088268\tSTD:0.0322362564055037\n",
      "\n",
      "Category Averages:\n",
      "                      precision  recall  f1-score  support\n",
      "label                                                     \n",
      "Action                    0.870   0.900     0.885     87.2\n",
      "ActionInput               0.790   0.904     0.841     37.1\n",
      "Amount                    0.920   0.939     0.928     71.4\n",
      "Atmosphere_Condition      0.808   0.851     0.825     11.0\n",
      "Data                      0.598   0.655     0.620      2.5\n",
      "Lighting                  0.150   0.058     0.083      1.1\n",
      "Product                   0.367   0.291     0.303      1.5\n",
      "Product_ref               0.000   0.000     0.000      0.1\n",
      "Reagent                   0.891   0.878     0.883     48.8\n",
      "RemovedChemical           0.590   0.638     0.600      6.0\n",
      "RemovedChemical_ref       0.000   0.000     0.000      0.2\n",
      "Repetition_Condition      0.306   0.321     0.313      2.7\n"
     ]
    }
   ],
   "source": [
    "# Print report \n",
    "import csv\n",
    "print('10-fold Cross Validation Results:')\n",
    "print()\n",
    "print('Overall Statistics:')\n",
    "print()\n",
    "print(f'F1-SCORE:\\tMEAN: {f1_avg}\\tSTD:{f1_std}')\n",
    "print(f'PRECISION:\\tMEAN: {precision_avg}\\tSTD:{precision_std}')\n",
    "print(f'RECALL:\\t\\tMEAN: {recall_avg}\\tSTD:{recall_std}')\n",
    "print()\n",
    "print('Category Averages:')\n",
    "print(report_avg)\n",
    "report_avg.to_csv('report.csv', quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
